---
title: "Blog_326"
listing:
  contents: posts
  sort: "date desc"
  type: default
  categories: true
  sort-ui: false
  filter-ui: false
page-layout: full
title-block-banner: true
---

hi

## Biggest Take-aways from my Mini-Projects this semester:

**Mini-Project 1**

Mini-Project 1 laid the groundwork for being able to do nearly everything that we did in Mathematical Statistics this year. In this project, we created sampling distributions for the sample minimum and sample maximum from Normal, Uniform, Beta, and Exponential distributions. These are the distributions that we used most often this semester, and it was important to be able to visually see them and conceptually understand them at the start of the semester, so that we could later incorporate them with things like MLE's, MOM's, confidence intervals, and hypothesis testing. A couple of take-aways from this project was that for Normal and Exponential Distributions, we expect $SE(Y_{min})$ and $SE(Y_{max})$ to be the same, whereas we expect $SE(Y_{max})$ to be significantly higher than $SE(Y_{min})$ for the Exponential distribution, and we expect $SE(Y_{min})$ to be significantly higher than $SE(Y_{max})$ for the Beta distribution.

**Mini-Project 2**

Mini-Project 2 asked us to write a meaningful story using the terms that we have been using in class. This was a fun mini-project, becuase it forced me to take classroom statistical concepts and apply them to a real-world setting, and I chose to utilize them in a story about one of my favorite things - sports! My biggest take-away from this mini-project was the similarities/differences with how we use words day-to-day and what they mean in a statistical context. Specifically, terms like variance and random sample mean essentially the same thing that you would expect, but terms like consistent and likelihood mean different things than we would expect! In a typical day-to-day setting, we would use consistent to mean a pattern that is repeated the same, and likelihood to mean the chances that something happens. In a mathematical statistics framework, we use consistency to mean that as n approaches infinity, the probability is 100% that our estimator produces a number very close to the true $\theta$, and likelihood refers to a function that is equal to a product of probability densities for each $x_i$. This mini-project helped me to conceptually fully understand all the things that we calculated and visualized in Mini-Project 1 more thoroughly.

**Mini-Project 3**

Mini-Project 3 was an important conceptual project. This mini-project showed us how changing sample sizes between small/medium/large and p close and far from 0.5 changed coverage rates and average interval widths. It incorporated running a large number of simulations which we have done in units throughout the semester, and we looked at sampling distributions as well which is also a fundamental concept to most units in Mathematical Statistics this semester. This project demonstrated with real data an important idea: if we have a large enough n, our confidence interval coverage rate will be extremely close to our confidence level, and the average interval width will be smaller. Utilizing a small n sacrifices on coverage rate, and leads to wider interval widths. We discussed confidence intervals and an article relating to p-values in Mini-Project 5.

**Mini-Project 4**

Mini-Project 4 was more of a "stand-alone" project compared to the other ones, because section 4 was the only section in which we thought about statistics within a Bayesian framework instead of with a frequentist approach. Specifically, in this project we looked at the likelihood that Nadal wins a point on his serve against Djokovic. This mini-project gave us experience doing the complete Bayesian analysis from start to finish by ourselves, using different priors and learning about how they affect the posteriors. One of the biggest takeaways from me was that the stronger the prior beliefs are, the higher the alpha and beta parameters will be, and the less the posterior will be updated. Unless you have a truly valid reason for having such strong prior beliefs, then I think it is smarter to stay on the more cautious side, so that the posterior is actually updated by real data. In this project we saw the commentator be extremely confident about the proportion of serves he knew Nadal was going to win, and the actual observed data essentially changed nothing in the posterior.

**Mini-Project 5**

For Mini-Project 5, we looked at an article that insisted that p-values should not hold nearly the weight that they do in statistics and in science. This mini-project conceptually related to every unit we have done in Mathematical Statistics, because each unit we have worked with either p-values, confidence levels, or other very specific statistical parameters that we have used to determine whether something was "significant" or not. This article also discusses that we should not just look at one statistic and let that be the sole determining factor in whether we say we have found something in research. Throughout this course, even though we have used p-values extremely frequently, we incorporate plots to visualize trends as well, and we usually use multiple statistics in our calculation process, instead of just a p-value. It is important to recognize that despite how much we use p-values in statistics, it is dangerous to use it as your only piece of evidence when considering publishing research.



